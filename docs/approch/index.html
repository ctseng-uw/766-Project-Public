<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-approch">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.18">
<title data-rh="true">Approach &amp; Implmentation | Style Transfer for Chinese Fonts </title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ctseng-uw.github.io//766-Project-Public/docs/approch"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Approach &amp; Implmentation | Style Transfer for Chinese Fonts "><meta data-rh="true" name="description" content="We consider the method of style transfer, which will adopt destination‚Äôs appearance to source‚Äôs images. The existing approach to do is Generative Adversarial Network(GAN). In GAN, we train 2 models simultaneously, one is generator, which will generate destination‚Äôs image from source input(the ‚Äúfake‚Äù destination image), and another is discriminator, which will classify the images as either the real destination image or the generated ‚Äúfake‚Äù image. There are several notable variants in GAN, including DCGan, pix2pix, CycleGan, etc."><meta data-rh="true" property="og:description" content="We consider the method of style transfer, which will adopt destination‚Äôs appearance to source‚Äôs images. The existing approach to do is Generative Adversarial Network(GAN). In GAN, we train 2 models simultaneously, one is generator, which will generate destination‚Äôs image from source input(the ‚Äúfake‚Äù destination image), and another is discriminator, which will classify the images as either the real destination image or the generated ‚Äúfake‚Äù image. There are several notable variants in GAN, including DCGan, pix2pix, CycleGan, etc."><link data-rh="true" rel="icon" href="/766-Project-Public/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ctseng-uw.github.io//766-Project-Public/docs/approch"><link data-rh="true" rel="alternate" href="https://ctseng-uw.github.io//766-Project-Public/docs/approch" hreflang="en"><link data-rh="true" rel="alternate" href="https://ctseng-uw.github.io//766-Project-Public/docs/approch" hreflang="x-default"><link rel="stylesheet" href="/766-Project-Public/assets/css/styles.9d8986b9.css">
<link rel="preload" href="/766-Project-Public/assets/js/runtime~main.25d20e1b.js" as="script">
<link rel="preload" href="/766-Project-Public/assets/js/main.9e118ba8.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/766-Project-Public/"><b class="navbar__title">CS766 Project</b></a><a class="navbar__item navbar__link navbar__link--active" href="/766-Project-Public/docs/motivation">Report</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/ctseng-uw/766-Project-Public" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_S7eR colorModeToggle_vKtC"><button class="clean-btn toggleButton_rCf9 toggleButtonDisabled_Pu9x" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_RiAD"><nav class="menu thin-scrollbar menu_izAj"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/766-Project-Public/docs/motivation">Motivation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/766-Project-Public/docs/approch">Approach &amp; Implmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/766-Project-Public/docs/results">Results &amp; Discussion</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/766-Project-Public/docs/conclusion">Conclusion &amp; Future work</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/766-Project-Public/docs/sourcecode">Source Code &amp; Dataset</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/766-Project-Public/docs/video">Presentation Slide &amp; Video</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/766-Project-Public/docs/proposal">Proposal</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/766-Project-Public/docs/midterm">Midterm Report</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/766-Project-Public/docs/reference">Reference</a></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Xlws" aria-label="breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/766-Project-Public/">üè†</a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="item name">Approach &amp; Implmentation</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Approach &amp; Implmentation</h1><p>We consider the method of style transfer, which will adopt destination‚Äôs appearance to source‚Äôs images. The existing approach to do is Generative Adversarial Network(GAN). In GAN, we train 2 models simultaneously, one is generator, which will generate destination‚Äôs image from source input(the ‚Äúfake‚Äù destination image), and another is discriminator, which will classify the images as either the real destination image or the generated ‚Äúfake‚Äù image. There are several notable variants in GAN, including DCGan, pix2pix, CycleGan, etc.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="attempt---pix2pix">Attempt - Pix2Pix<a class="hash-link" href="#attempt---pix2pix" title="Direct link to heading">‚Äã</a></h2><p>Pix2Pix uses conditional GAN as the solution to image-to-image translation problem, which needs paired (dst,src) as training input. Below are two examples that this model can effetively complete.</p><p align="center"><img loading="lazy" style="width:900px" src="/766-Project-Public/assets/images/pix2pix-04b6c429aec6cda754e5834651fa7070.png" class="img_E7b_"></p><figcaption>The pix2pix result examples[2]</figcaption><p></p><p>However, this method requires &quot;pixel to pixel&quot; mapping, so it can work well if the font style is not significantly different but is unusable in handwriting style font, where the pixel has no direct relation to.</p><p align="center"><img loading="lazy" style="width:900px" src="/766-Project-Public/assets/images/pix2pix_font-6f5e64bfcd8e036d68ccbdd2666f3f51.png" class="img_E7b_"></p><figcaption>font style transfer attempt result with pix2pix</figcaption><p></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="dataset">Dataset<a class="hash-link" href="#dataset" title="Direct link to heading">‚Äã</a></h2><p>Dataset is generated from existing font file (<code>.ttf</code>).</p><p align="center"><img loading="lazy" style="width:900px" src="/766-Project-Public/assets/images/dataset_fonts-8e2a6829ddfe609e5702decf7d1f345e.png" class="img_E7b_"></p><figcaption>Dataset example with 5 different fonts</figcaption><p></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="implemented-models">Implemented models<a class="hash-link" href="#implemented-models" title="Direct link to heading">‚Äã</a></h2><p>We implemented the following models using pytorch. We also unified the generator and discriminator&#x27;s interface, so different generator and discriminators can be hot-plugged to the code.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="1-cyclegan-and-discogan">1. CycleGAN and DiscoGAN<a class="hash-link" href="#1-cyclegan-and-discogan" title="Direct link to heading">‚Äã</a></h3><p>First we implemented a baseline Cycle Gan model. Cycle Gan is an unpaired image-to-image translation method proposed by Zhu<!-- -->[2]<!-- --> etc. It contains <strong>2 Generators</strong>, one generated the font from source font image to destination font image(the &quot;fake&quot; destination font), and another generated from destination to source(the &quot;fake&quot; source font), and <strong>2 Discriminators</strong> classifying the destination font and the source font. In cycleGan, it will use the &quot;fake&quot; destination font to generate a <strong>&quot;fake fake&quot; source font</strong>, and using this font and <strong>the original source font</strong>(the &quot;real&quot; source) to calculate L1 loss as the forward cycle-consistency loss, since we would hope the fake fake source is close to the real source font. Similarly, there is a backward cycle-consistency loss calculating between the fake fake destination(genereated by fake source font) and the real destination.</p><p align="center"><img loading="lazy" style="width:700px" src="/766-Project-Public/assets/images/cyclegan-8a04b5e40c794339fc8fb7cccd30918d.png" class="img_E7b_"></p><figcaption>The cycleGAN model structure[3]</figcaption><p></p><p>There‚Äôs another similar GAN, the discoGAN, using the same idea, but with different model structure as the below image showed, and it utilized the mean square error loss to replace the L1 loss.</p><p align="center"><img loading="lazy" style="width:300px" src="/766-Project-Public/assets/images/discogan-fe2243c57c8b2abc5cb3cb880e1583e6.png" class="img_E7b_"></p><figcaption>The discoGAN model structure[4]</figcaption><p></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="2-dualgan">2. DualGAN<a class="hash-link" href="#2-dualgan" title="Direct link to heading">‚Äã</a></h3><p>We then attemped to plug-in DualGAN&#x27;s generator and discriminator. DualGAN is a model purposed by Yi<!-- -->[1]<!-- --> etc. For the generator part, it adopts the U-shape net structure. This helps the model to share low-level information and keep the alignment of the image structures.</p><p>For the discriminator part, it employs the Markovian PatchGAN model. It keeps the independence between pixels distanced beyond a specific patch size and it is effective in capturing ocal high frequency.</p><p>All above characteristics of DualGAN contributes to a more stable output structure. However, too much constraints may also lead to the underfitting problem.</p><p align="center"><img loading="lazy" style="width:900px" src="/766-Project-Public/assets/images/dualGAN-f0880b8b23b46e8365f153a7fc056ec2.png" class="img_E7b_"></p><figcaption>The dualGAN model structure[1]</figcaption><p></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="3-our-methods">3. Our methods<a class="hash-link" href="#3-our-methods" title="Direct link to heading">‚Äã</a></h3><p>Now we will describe our modification. We decide to modify based on CycleGAN since it performed the best based on our previous experiments. Some notable modifications include changing the loss function to MSE as in DiscoGAN since it is more sensitive to shape changes (again based on our previous experiments). We also noticed that it is easy to generate paired data in our use case so we introduced a &quot;true loss&quot; to the loss function, which is the mse compared to the ground truth. Additionally, we also add some preprocessing including random flips and converting images to grayscale.</p><h1>References</h1><ol><li>Yi, Zili &amp; Zhang, Hao &amp; Tan, Ping &amp; Gong, Minglun. (2017). DualGAN: Unsupervised Dual Learning for Image-to-Image Translation. 2868-2876. 10.1109/ICCV.2017.310.</li><li>Zhu, J.-Y., Park, T., Isola, P., <!-- -->&amp;<!-- --> Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. 2017 IEEE International Conference on Computer Vision (ICCV).</li></ol></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/766-Project-Public/docs/motivation"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Motivation</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/766-Project-Public/docs/results"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Results &amp; Discussion</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#attempt---pix2pix" class="table-of-contents__link toc-highlight">Attempt - Pix2Pix</a></li><li><a href="#dataset" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#implemented-models" class="table-of-contents__link toc-highlight">Implemented models</a><ul><li><a href="#1-cyclegan-and-discogan" class="table-of-contents__link toc-highlight">1. CycleGAN and DiscoGAN</a></li><li><a href="#2-dualgan" class="table-of-contents__link toc-highlight">2. DualGAN</a></li><li><a href="#3-our-methods" class="table-of-contents__link toc-highlight">3. Our methods</a></li></ul></li></ul></div></div></div></div></main></div></div></div>
<script src="/766-Project-Public/assets/js/runtime~main.25d20e1b.js"></script>
<script src="/766-Project-Public/assets/js/main.9e118ba8.js"></script>
</body>
</html>