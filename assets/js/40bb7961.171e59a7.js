"use strict";(self.webpackChunkcs_766_website=self.webpackChunkcs_766_website||[]).push([[784],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return f}});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(n),f=i,h=d["".concat(l,".").concat(f)]||d[f]||u[f]||r;return n?a.createElement(h,o(o({ref:t},p),{},{components:n})):a.createElement(h,o({ref:t},p))}));function f(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},5305:function(e,t,n){n.r(t),n.d(t,{assets:function(){return p},contentTitle:function(){return l},default:function(){return f},frontMatter:function(){return s},metadata:function(){return c},toc:function(){return u}});var a=n(7462),i=n(3366),r=(n(7294),n(3905)),o=["components"],s={},l="Approach & Implmentation",c={unversionedId:"approch",id:"approch",title:"Approach & Implmentation",description:"We consider the method of style transfer, which will adopt destination\u2019s appearance to source\u2019s images. The existing approach to do is Generative Adversarial Network(GAN). In GAN, we train 2 models simultaneously, one is generator, which will generate destination\u2019s image from source input(the \u201cfake\u201d destination image), and another is discriminator, which will classify the images as either the real destination image or the generated \u201cfake\u201d image. There are several notable variants in GAN, including DCGan, pix2pix, CycleGan, etc.",source:"@site/docs/2-approch.md",sourceDirName:".",slug:"/approch",permalink:"/766-Project-Public/docs/approch",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Motivation",permalink:"/766-Project-Public/docs/motivation"},next:{title:"Results",permalink:"/766-Project-Public/docs/results"}},p={},u=[{value:"Attempt - Pix2Pix",id:"attempt---pix2pix",level:2},{value:"Dataset",id:"dataset",level:2},{value:"1. CycleGAN and DiscoGAN",id:"1-cyclegan-and-discogan",level:3},{value:"2. DualGAN",id:"2-dualgan",level:3},{value:"3. Our methods",id:"3-our-methods",level:3}],d={toc:u};function f(e){var t=e.components,s=(0,i.Z)(e,o);return(0,r.kt)("wrapper",(0,a.Z)({},d,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"approach--implmentation"},"Approach & Implmentation"),(0,r.kt)("p",null,"We consider the method of style transfer, which will adopt destination\u2019s appearance to source\u2019s images. The existing approach to do is Generative Adversarial Network(GAN). In GAN, we train 2 models simultaneously, one is generator, which will generate destination\u2019s image from source input(the \u201cfake\u201d destination image), and another is discriminator, which will classify the images as either the real destination image or the generated \u201cfake\u201d image. There are several notable variants in GAN, including DCGan, pix2pix, CycleGan, etc. "),(0,r.kt)("h2",{id:"attempt---pix2pix"},"Attempt - Pix2Pix"),(0,r.kt)("p",null,"Pix2Pix uses conditional GAN as the solution to image-to-image translation problem, which needs paired (dst,src) as training input. Below are two examples that this model can effetively complete."),(0,r.kt)("p",{align:"center"},(0,r.kt)("img",{style:{width:900},src:n(8305).Z}),(0,r.kt)("figcaption",null,"The pix2pix result examples[2]")),(0,r.kt)("p",null,'However, this method requires "pixel to pixel" mapping, so it can work well if the font style is not significantly different but is unusable in handwriting style font, where the pixel has no direct relation to.'),(0,r.kt)("p",{align:"center"},(0,r.kt)("img",{style:{width:900},src:n(8665).Z}),(0,r.kt)("figcaption",null,"font style transfer attempt result with pix2pix")),(0,r.kt)("h2",{id:"dataset"},"Dataset"),(0,r.kt)("p",{align:"center"},(0,r.kt)("img",{style:{width:900},src:n(9047).Z}),(0,r.kt)("figcaption",null,"Dataset example with 5 different fonts")),(0,r.kt)("h3",{id:"1-cyclegan-and-discogan"},"1. CycleGAN and DiscoGAN"),(0,r.kt)("p",null,"Cycle Gan is an unpaired image-to-image translation method proposed by Zhu","[2]"," etc. It contains ",(0,r.kt)("strong",{parentName:"p"},"2 Generators"),', one generated the font from source font image to destination font image(the "fake" destination font), and another generated from destination to source(the "fake" source font), and ',(0,r.kt)("strong",{parentName:"p"},"2 Discriminators"),' classifying the destination font and the source font. In cycleGan, it will use the "fake" destination font to generate a ',(0,r.kt)("strong",{parentName:"p"},'"fake fake" source font'),", and using this font and ",(0,r.kt)("strong",{parentName:"p"},"the original source font"),'(the "real" source) to calculate L1 loss as the forward cycle-consistency loss, since we would hope the fake fake source is close to the real source font. Similarly, there is a backward cycle-consistency loss calculating between the fake fake destination(genereated by fake source font) and the real destination. '),(0,r.kt)("p",{align:"center"},(0,r.kt)("img",{style:{width:700},src:n(6876).Z}),(0,r.kt)("figcaption",null,"The cycleGAN model structure[3]")),(0,r.kt)("p",null,"There\u2019s another similar GAN, the discoGAN, using the same idea, but with different model structure as the below image showed, and it utilized the mean square error loss to replace the L1 loss."),(0,r.kt)("p",{align:"center"},(0,r.kt)("img",{style:{width:300},src:n(7948).Z}),(0,r.kt)("figcaption",null,"The discoGAN model structure[4]")),(0,r.kt)("h3",{id:"2-dualgan"},"2. DualGAN"),(0,r.kt)("p",null,"DualGAN is a model purposed by Yi","[1]"," etc. For the generator part, it adopts the U-shape net structure. This helps the model to share low-level information and keep the alignment of the image structures."),(0,r.kt)("p",null,"For the discriminator part, it employs the Markovian PatchGAN model. It keeps the independence between pixels distanced beyond a specific patch size and it is effective in capturing ocal high frequency."),(0,r.kt)("p",null,"All above characteristics of DualGAN contributes to a more stable output structure. However, too much constraints may also lead to the underfitting problem."),(0,r.kt)("p",{align:"center"},(0,r.kt)("img",{style:{width:900},src:n(9196).Z}),(0,r.kt)("figcaption",null,"The dualGAN model structure[1]")),(0,r.kt)("h3",{id:"3-our-methods"},"3. Our methods"),(0,r.kt)("h1",{id:"references"},"References"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Yi, Zili & Zhang, Hao & Tan, Ping & Gong, Minglun. (2017). DualGAN: Unsupervised Dual Learning for Image-to-Image Translation. 2868-2876. 10.1109/ICCV.2017.310."),(0,r.kt)("li",{parentName:"ol"},"Zhu, J.-Y., Park, T., Isola, P., ","&"," Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. 2017 IEEE International Conference on Computer Vision (ICCV).")))}f.isMDXComponent=!0},6876:function(e,t,n){t.Z=n.p+"assets/images/cyclegan-8a04b5e40c794339fc8fb7cccd30918d.png"},9047:function(e,t,n){t.Z=n.p+"assets/images/dataset_fonts-8e2a6829ddfe609e5702decf7d1f345e.png"},7948:function(e,t,n){t.Z=n.p+"assets/images/discogan-fe2243c57c8b2abc5cb3cb880e1583e6.png"},9196:function(e,t,n){t.Z=n.p+"assets/images/dualGAN-f0880b8b23b46e8365f153a7fc056ec2.png"},8305:function(e,t,n){t.Z=n.p+"assets/images/pix2pix-04b6c429aec6cda754e5834651fa7070.png"},8665:function(e,t,n){t.Z=n.p+"assets/images/pix2pix_font-6f5e64bfcd8e036d68ccbdd2666f3f51.png"}}]);