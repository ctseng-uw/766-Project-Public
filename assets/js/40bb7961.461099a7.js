"use strict";(self.webpackChunkcs_766_website=self.webpackChunkcs_766_website||[]).push([[784],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return d}});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=r.createContext({}),u=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=u(e.components);return r.createElement(l.Provider,{value:t},e.children)},s={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,p=c(e,["components","mdxType","originalType","parentName"]),f=u(n),d=o,m=f["".concat(l,".").concat(d)]||f[d]||s[d]||a;return n?r.createElement(m,i(i({ref:t},p),{},{components:n})):r.createElement(m,i({ref:t},p))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=f;var c={};for(var l in t)hasOwnProperty.call(t,l)&&(c[l]=t[l]);c.originalType=e,c.mdxType="string"==typeof e?e:o,i[1]=c;for(var u=2;u<a;u++)i[u]=n[u];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}f.displayName="MDXCreateElement"},5305:function(e,t,n){n.r(t),n.d(t,{assets:function(){return p},contentTitle:function(){return l},default:function(){return d},frontMatter:function(){return c},metadata:function(){return u},toc:function(){return s}});var r=n(7462),o=n(3366),a=(n(7294),n(3905)),i=["components"],c={},l="Approach & Implmentation",u={unversionedId:"approch",id:"approch",title:"Approach & Implmentation",description:"1. CycleGAN",source:"@site/docs/2-approch.md",sourceDirName:".",slug:"/approch",permalink:"/766-Project-Public/docs/approch",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Motivation",permalink:"/766-Project-Public/docs/motivation"},next:{title:"Results",permalink:"/766-Project-Public/docs/results"}},p={},s=[{value:"1. CycleGAN",id:"1-cyclegan",level:3},{value:"2. DiscoGAN",id:"2-discogan",level:3},{value:"3. DualGAN",id:"3-dualgan",level:3},{value:"4. Our methods",id:"4-our-methods",level:3}],f={toc:s};function d(e){var t=e.components,c=(0,o.Z)(e,i);return(0,a.kt)("wrapper",(0,r.Z)({},f,c,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"approach--implmentation"},"Approach & Implmentation"),(0,a.kt)("h3",{id:"1-cyclegan"},"1. CycleGAN"),(0,a.kt)("h3",{id:"2-discogan"},"2. DiscoGAN"),(0,a.kt)("h3",{id:"3-dualgan"},"3. DualGAN"),(0,a.kt)("p",null,"DualGAN is a model purposed by Yi","[1]"," etc. For the generator part, it adopts the U-shape net structure. This helps the model to share low-level information and keep the alignment of the image structures."),(0,a.kt)("p",null,"For the discriminator part, it employs the Markovian PatchGAN model. It keeps the independence between pixels distanced beyond a specific patch size and it is effective in capturing ocal high frequency."),(0,a.kt)("p",null,"All above characteristics of DualGAN contributes to a more stable output structure. However, too much constraints may also lead to the underfitting problem."),(0,a.kt)("p",{align:"center"},(0,a.kt)("img",{style:{width:900},src:n(9196).Z}),(0,a.kt)("figcaption",null,"The dualGAN model structure[1]")),(0,a.kt)("h3",{id:"4-our-methods"},"4. Our methods"),(0,a.kt)("h1",{id:"references"},"References"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Yi, Zili & Zhang, Hao & Tan, Ping & Gong, Minglun. (2017). DualGAN: Unsupervised Dual Learning for Image-to-Image Translation. 2868-2876. 10.1109/ICCV.2017.310.")))}d.isMDXComponent=!0},9196:function(e,t,n){t.Z=n.p+"assets/images/dualGAN-f0880b8b23b46e8365f153a7fc056ec2.png"}}]);