"use strict";(self.webpackChunkcs_766_website=self.webpackChunkcs_766_website||[]).push([[784],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return m}});var i=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=i.createContext({}),c=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=c(e.components);return i.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=c(n),m=a,f=u["".concat(l,".").concat(m)]||u[m]||p[m]||o;return n?i.createElement(f,r(r({ref:t},d),{},{components:n})):i.createElement(f,r({ref:t},d))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,r=new Array(o);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,r[1]=s;for(var c=2;c<o;c++)r[c]=n[c];return i.createElement.apply(null,r)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},5305:function(e,t,n){n.r(t),n.d(t,{assets:function(){return d},contentTitle:function(){return l},default:function(){return m},frontMatter:function(){return s},metadata:function(){return c},toc:function(){return p}});var i=n(7462),a=n(3366),o=(n(7294),n(3905)),r=["components"],s={},l="Approach & Implmentation",c={unversionedId:"approch",id:"approch",title:"Approach & Implmentation",description:"We consider the method of style transfer, which will adopt destination\u2019s appearance to source\u2019s images. The existing approach to do is Generative Adversarial Network(GAN). In GAN, we train 2 models simultaneously, one is generator, which will generate destination\u2019s image from source input(the \u201cfake\u201d destination image), and another is discriminator, which will classify the images as either the real destination image or the generated \u201cfake\u201d image. There are several notable variants in GAN, including DCGan, pix2pix, CycleGan, etc.",source:"@site/docs/2-approch.md",sourceDirName:".",slug:"/approch",permalink:"/766-Project-Public/docs/approch",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Motivation",permalink:"/766-Project-Public/docs/motivation"},next:{title:"Results & Discussion",permalink:"/766-Project-Public/docs/results"}},d={},p=[{value:"Attempt - Pix2Pix",id:"attempt---pix2pix",level:2},{value:"Dataset",id:"dataset",level:2},{value:"Implemented models",id:"implemented-models",level:2},{value:"1. CycleGAN and DiscoGAN",id:"1-cyclegan-and-discogan",level:3},{value:"2. DualGAN",id:"2-dualgan",level:3},{value:"3. Our methods",id:"3-our-methods",level:3}],u={toc:p};function m(e){var t=e.components,s=(0,a.Z)(e,r);return(0,o.kt)("wrapper",(0,i.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"approach--implmentation"},"Approach & Implmentation"),(0,o.kt)("p",null,"We consider the method of style transfer, which will adopt destination\u2019s appearance to source\u2019s images. The existing approach to do is Generative Adversarial Network(GAN). In GAN, we train 2 models simultaneously, one is generator, which will generate destination\u2019s image from source input(the \u201cfake\u201d destination image), and another is discriminator, which will classify the images as either the real destination image or the generated \u201cfake\u201d image. There are several notable variants in GAN, including DCGan, pix2pix, CycleGan, etc."),(0,o.kt)("h2",{id:"attempt---pix2pix"},"Attempt - Pix2Pix"),(0,o.kt)("p",null,"Pix2Pix uses conditional GAN as the solution to image-to-image translation problem, which needs paired (dst,src) as training input. Below are two examples that this model can effetively complete."),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{style:{width:900},src:n(8305).Z}),(0,o.kt)("figcaption",null,"The pix2pix result examples[2]")),(0,o.kt)("p",null,'However, this method requires "pixel to pixel" mapping, so it can work well if the font style is not significantly different but is unusable in handwriting style font, where the pixel has no direct relation to.'),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{style:{width:900},src:n(8665).Z}),(0,o.kt)("figcaption",null,"font style transfer attempt result with pix2pix")),(0,o.kt)("h2",{id:"dataset"},"Dataset"),(0,o.kt)("p",null,"Dataset is generated from existing font file (",(0,o.kt)("inlineCode",{parentName:"p"},".ttf"),")."),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{style:{width:900},src:n(9047).Z}),(0,o.kt)("figcaption",null,"Dataset example with 5 different fonts")),(0,o.kt)("h2",{id:"implemented-models"},"Implemented models"),(0,o.kt)("p",null,"We implemented the following models using pytorch. We also unified the generator and discriminator's interface, so different generator and discriminators can be hot-plugged to the code."),(0,o.kt)("h3",{id:"1-cyclegan-and-discogan"},"1. CycleGAN and DiscoGAN"),(0,o.kt)("p",null,"First we implemented a baseline Cycle Gan model. Cycle Gan is an unpaired image-to-image translation method proposed by Zhu","[2]"," etc. It contains ",(0,o.kt)("strong",{parentName:"p"},"2 Generators"),', one generated the font from source font image to destination font image(the "fake" destination font), and another generated from destination to source(the "fake" source font), and ',(0,o.kt)("strong",{parentName:"p"},"2 Discriminators"),' classifying the destination font and the source font. In cycleGan, it will use the "fake" destination font to generate a ',(0,o.kt)("strong",{parentName:"p"},'"fake fake" source font'),", and using this font and ",(0,o.kt)("strong",{parentName:"p"},"the original source font"),'(the "real" source) to calculate L1 loss as the forward cycle-consistency loss, since we would hope the fake fake source is close to the real source font. Similarly, there is a backward cycle-consistency loss calculating between the fake fake destination(genereated by fake source font) and the real destination.'),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{style:{width:700},src:n(6876).Z}),(0,o.kt)("figcaption",null,"The cycleGAN model structure[3]")),(0,o.kt)("p",null,"There\u2019s another similar GAN, the discoGAN, using the same idea, but with different model structure as the below image showed, and it utilized the mean square error loss to replace the L1 loss."),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{style:{width:300},src:n(7948).Z}),(0,o.kt)("figcaption",null,"The discoGAN model structure[4]")),(0,o.kt)("h3",{id:"2-dualgan"},"2. DualGAN"),(0,o.kt)("p",null,"We then attemped to plug-in DualGAN's generator and discriminator. DualGAN is a model purposed by Yi","[1]"," etc. For the generator part, it adopts the U-shape net structure. This helps the model to share low-level information and keep the alignment of the image structures."),(0,o.kt)("p",null,"For the discriminator part, it employs the Markovian PatchGAN model. It keeps the independence between pixels distanced beyond a specific patch size and it is effective in capturing ocal high frequency."),(0,o.kt)("p",null,"All above characteristics of DualGAN contributes to a more stable output structure. However, too much constraints may also lead to the underfitting problem."),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{style:{width:900},src:n(9196).Z}),(0,o.kt)("figcaption",null,"The dualGAN model structure[1]")),(0,o.kt)("h3",{id:"3-our-methods"},"3. Our methods"),(0,o.kt)("p",null,'Now we will describe our modification. We decide to modify based on CycleGAN since it performed the best based on our previous experiments. Some notable modifications include changing the loss function to MSE as in DiscoGAN since it is more sensitive to shape changes (again based on our previous experiments). We also noticed that it is easy to generate paired data in our use case so we introduced a "true loss" to the loss function, which is the mse compared to the ground truth. Additionally, we also add some preprocessing including random flips and converting images to grayscale.'),(0,o.kt)("h1",{id:"references"},"References"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Yi, Zili & Zhang, Hao & Tan, Ping & Gong, Minglun. (2017). DualGAN: Unsupervised Dual Learning for Image-to-Image Translation. 2868-2876. 10.1109/ICCV.2017.310."),(0,o.kt)("li",{parentName:"ol"},"Zhu, J.-Y., Park, T., Isola, P., ","&"," Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. 2017 IEEE International Conference on Computer Vision (ICCV).")))}m.isMDXComponent=!0},6876:function(e,t,n){t.Z=n.p+"assets/images/cyclegan-8a04b5e40c794339fc8fb7cccd30918d.png"},9047:function(e,t,n){t.Z=n.p+"assets/images/dataset_fonts-8e2a6829ddfe609e5702decf7d1f345e.png"},7948:function(e,t,n){t.Z=n.p+"assets/images/discogan-fe2243c57c8b2abc5cb3cb880e1583e6.png"},9196:function(e,t,n){t.Z=n.p+"assets/images/dualGAN-f0880b8b23b46e8365f153a7fc056ec2.png"},8305:function(e,t,n){t.Z=n.p+"assets/images/pix2pix-04b6c429aec6cda754e5834651fa7070.png"},8665:function(e,t,n){t.Z=n.p+"assets/images/pix2pix_font-6f5e64bfcd8e036d68ccbdd2666f3f51.png"}}]);